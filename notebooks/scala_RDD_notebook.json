{"paragraphs":[{"text":"import org.apache.spark.sql.expressions.Window\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.Row\n\n//variable de path del csv\nval data =\"/data/slice_violations.csv\"\n//constante para hacer el join entre data frames\nval JOIN_COLUMN: String = \"rn\"\n\n//definicion de una funcion que aplica un join entre dos dataframes y devuelve\n//otro data frame como resultado\ndef joinDataFrames(df: DataFrame, df2: DataFrame,joinCol: String) : DataFrame = {\n    df.join(df2, Seq(joinCol))\n}\n\n//funcion para filtrar por los accidentes donde hubo heridos utilizando un iterador\ndef filterInjuried(numbers: Iterator[Row]) : Iterator[Row] = \n  numbers.filter(row => row(10) == \"true\")\n\n//funcion para el filtrado y formateo del dataframe resultado utilizando map partitions\ndef filterMapPartitions(df: DataFrame) : DataFrame = {\n     val dfRES = df.rdd.mapPartitions(filterInjuried).\n     map(row => (row.getString(21), row.getString(22), row.getString(23))).\n     toDF(\"Mark\", \"Model\", \"Color\")\n     dfRES\n}\n\n//funcion para filtrar por accidentes donde hubo heridos y darle formato\ndef filterDF(df: DataFrame) : DataFrame = {\n     val dfRES = df.filter(df(\"personal_injury\") === \"true\").\n     map(row => (row.getString(21), row.getString(22), row.getString(23))).\n     toDF(\"Mark\", \"Model\", \"Color\")\n     dfRES\n}","dateUpdated":"2018-11-05T12:45:20+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.expressions.Window\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.Row\ndata: String = /data/slice_violations.csv\nJOIN_COLUMN: String = rn\njoinDataFrames: (df: org.apache.spark.sql.DataFrame, df2: org.apache.spark.sql.DataFrame, joinCol: String)org.apache.spark.sql.DataFrame\nfilterInjuried: (numbers: Iterator[org.apache.spark.sql.Row])Iterator[org.apache.spark.sql.Row]\nfilterMapPartitions: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\nfilterDF: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"}]},"apps":[],"jobName":"paragraph_1541421639786_-704266222","id":"20181102-082048_2130822318","dateCreated":"2018-11-05T12:40:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1696","user":"anonymous","dateFinished":"2018-11-05T12:45:34+0000","dateStarted":"2018-11-05T12:45:20+0000"},{"text":"//dataframe con los datos cargados\nval df = spark.read.format(\"csv\").option(\"header\", \"true\").load(data)\ndf.printSchema()\ndf.limit(1).show()","dateUpdated":"2018-11-05T12:45:20+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"df: org.apache.spark.sql.DataFrame = [date_of_stop: string, time_of_stop: string ... 33 more fields]\nroot\n |-- date_of_stop: string (nullable = true)\n |-- time_of_stop: string (nullable = true)\n |-- agency: string (nullable = true)\n |-- subagency: string (nullable = true)\n |-- description: string (nullable = true)\n |-- location: string (nullable = true)\n |-- latitude: string (nullable = true)\n |-- longitude: string (nullable = true)\n |-- accident: string (nullable = true)\n |-- belts: string (nullable = true)\n |-- personal_injury: string (nullable = true)\n |-- property_damage: string (nullable = true)\n |-- fatal: string (nullable = true)\n |-- commercial_license: string (nullable = true)\n |-- hazmat: string (nullable = true)\n |-- commercial_vehicle: string (nullable = true)\n |-- alcohol: string (nullable = true)\n |-- work_zone: string (nullable = true)\n |-- state: string (nullable = true)\n |-- vehicletype: string (nullable = true)\n |-- year: string (nullable = true)\n |-- make: string (nullable = true)\n |-- model: string (nullable = true)\n |-- color: string (nullable = true)\n |-- violation_type: string (nullable = true)\n |-- charge: string (nullable = true)\n |-- article: string (nullable = true)\n |-- contributed_to_accident: string (nullable = true)\n |-- race: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- driver_city: string (nullable = true)\n |-- driver_state: string (nullable = true)\n |-- dl_state: string (nullable = true)\n |-- arrest_type: string (nullable = true)\n |-- geolocation: string (nullable = true)\n\n+------------+------------+------+--------------------+--------------------+---------------+--------+---------+--------+-----+---------------+---------------+-----+------------------+------+------------------+-------+---------+-----+---------------+----+----+-----+-----+--------------+---------+--------------------+-----------------------+-----+------+-----------+------------+--------+-----------------+-----------+\n|date_of_stop|time_of_stop|agency|           subagency|         description|       location|latitude|longitude|accident|belts|personal_injury|property_damage|fatal|commercial_license|hazmat|commercial_vehicle|alcohol|work_zone|state|    vehicletype|year|make|model|color|violation_type|   charge|             article|contributed_to_accident| race|gender|driver_city|driver_state|dl_state|      arrest_type|geolocation|\n+------------+------------+------+--------------------+--------------------+---------------+--------+---------+--------+-----+---------------+---------------+-----+------------------+------+------------------+-------+---------+-----+---------------+----+----+-----+-----+--------------+---------+--------------------+-----------------------+-----+------+-----------+------------+--------+-----------------+-----------+\n|  2013-09-24|    17:11:00|   MCP|3rd district, Sil...|DRIVING VEHICLE O...|8804 FLOWER AVE|    null|     null|   false|false|          false|          false|false|             false| false|             false|  false|    false|   MD|02 - Automobile|2008|FORD|   4S|BLACK|      Citation|13-401(h)|Transportation Ar...|                  false|BLACK|     M|TAKOMA PARK|          MD|      MD|A - Marked Patrol|       null|\n+------------+------------+------+--------------------+--------------------+---------------+--------+---------+--------+-----+---------------+---------------+-----+------------------+------+------------------+-------+---------+-----+---------------+----+----+-----+-----+--------------+---------+--------------------+-----------------------+-----+------+-----------+------------+--------+-----------------+-----------+\n\n"}]},"apps":[],"jobName":"paragraph_1541421639787_-704650971","id":"20181102-082109_189910193","dateCreated":"2018-11-05T12:40:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1697","user":"anonymous","dateFinished":"2018-11-05T12:45:39+0000","dateStarted":"2018-11-05T12:45:21+0000"},{"text":"//val dfFiltered=filterMapPartitions(df) //filtrado utilizando mapPartitions\nval dfFiltered=filterDF(df)  //filtrado utilizando un filter sobre el data frame\n\nval groupByRDD = dfFiltered.rdd.\nmap(x => ((x.get(0),x.get(1),x.get(2)).toString, 1)). //mapeamos marca, modelo y color (KEY) a 1 (VALUE)\nreduceByKey{case (x, y) => x + y}. //Hacemos un reduce by key\nmap(r => (r._2,r._1)). //damos la vuelta a las columnas, para que primero aparezca el count y despues la marca,modelo y color\nsortBy(r => (-r._1, r._2)).// ordenamos por la primera columna (count) de manera descendente y por la segunda de manera ascendente\ntoDF(\"Count\",\"Mark - Model - Color\").persist //convertimos el RDD a DataFrame aplicando \"persist\" para que lo cargue en memoria, ya que vamos a hacer\n                                             //varias operaciones con el\n\ngroupByRDD.show()","dateUpdated":"2018-11-05T12:45:20+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"dfFiltered: org.apache.spark.sql.DataFrame = [Mark: string, Model: string ... 1 more field]\ngroupByRDD: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Count: int, Mark - Model - Color: string]\n+-----+--------------------+\n|Count|Mark - Model - Color|\n+-----+--------------------+\n|    6|  (SUNNY,NINGBO,RED)|\n|    5|(ACURA,INTEGRA,BL...|\n|    5|(CHRYSLER,SEBRING...|\n|    5|(FORD,EXPLORER,GR...|\n|    5|  (HONDA,CIVIC,GRAY)|\n|    5|  (MITS,LANCER,GRAY)|\n|    4|(CHRYSLER,PACIFIC...|\n|    4|  (DODGE,DAKOTA,RED)|\n|    4|  (HONDA,PILOT,GRAY)|\n|    4|   (KIA,TRUCK,BLACK)|\n|    3|   (ACUR,RDX,SILVER)|\n|    3| (HONDA,ACCORD,BLUE)|\n|    3|(INFINITI,SEDAN,G...|\n|    3| (TOYO,CAMRY,SILVER)|\n|    3|(TOYOTA,COROLLA,S...|\n|    3|(TOYOTA,COROLLA,TAN)|\n|    2|     (CHEV,SU,BLACK)|\n|    2|(FORD,EXPEDITION,...|\n|    2|     (HOND,4S,BLACK)|\n|    2| (HONDA,CIVIC,BLACK)|\n+-----+--------------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1541421639788_-706574715","id":"20181102-082153_469239170","dateCreated":"2018-11-05T12:40:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1698","user":"anonymous","dateFinished":"2018-11-05T12:45:49+0000","dateStarted":"2018-11-05T12:45:34+0000"},{"text":"//aplicamos un DENSE_RANK para ver que posiciones en el ranking\nval windowSpecDR = Window.orderBy(col(\"Count\").desc)\nval dfDataDR = groupByRDD.withColumn(\"dr\", dense_rank().over(windowSpecDR)) \ndfDataDR.show","dateUpdated":"2018-11-05T12:45:21+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"windowSpecDR: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@5bc3e2e4\ndfDataDR: org.apache.spark.sql.DataFrame = [Count: int, Mark - Model - Color: string ... 1 more field]\n+-----+--------------------+---+\n|Count|Mark - Model - Color| dr|\n+-----+--------------------+---+\n|    6|  (SUNNY,NINGBO,RED)|  1|\n|    5|(ACURA,INTEGRA,BL...|  2|\n|    5|(CHRYSLER,SEBRING...|  2|\n|    5|(FORD,EXPLORER,GR...|  2|\n|    5|  (HONDA,CIVIC,GRAY)|  2|\n|    5|  (MITS,LANCER,GRAY)|  2|\n|    4|(CHRYSLER,PACIFIC...|  3|\n|    4|  (DODGE,DAKOTA,RED)|  3|\n|    4|  (HONDA,PILOT,GRAY)|  3|\n|    4|   (KIA,TRUCK,BLACK)|  3|\n|    3|   (ACUR,RDX,SILVER)|  4|\n|    3| (HONDA,ACCORD,BLUE)|  4|\n|    3|(INFINITI,SEDAN,G...|  4|\n|    3| (TOYO,CAMRY,SILVER)|  4|\n|    3|(TOYOTA,COROLLA,S...|  4|\n|    3|(TOYOTA,COROLLA,TAN)|  4|\n|    2|     (CHEV,SU,BLACK)|  5|\n|    2|(FORD,EXPEDITION,...|  5|\n|    2|     (HOND,4S,BLACK)|  5|\n|    2| (HONDA,CIVIC,BLACK)|  5|\n+-----+--------------------+---+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1541421639788_-706574715","id":"20181102-082239_1763662705","dateCreated":"2018-11-05T12:40:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1699","user":"anonymous","dateFinished":"2018-11-05T12:45:57+0000","dateStarted":"2018-11-05T12:45:40+0000"},{"text":"//añadimos al dataframe la columna \"rn\" aplicando un ROW_NUMBER() ordenando\n//por Count de manera descendente y \"marca -modelo-color\" ascendente\nval windowSpecRN = Window.orderBy(col(\"Count\").desc,col(\"Mark - Model - Color\"))\nval dfDataRN = groupByRDD.withColumn(\"rn\", row_number().over(windowSpecRN))\ndfDataRN.show()","dateUpdated":"2018-11-05T12:45:21+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"windowSpecRN: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@61a97a80\ndfDataRN: org.apache.spark.sql.DataFrame = [Count: int, Mark - Model - Color: string ... 1 more field]\n+-----+--------------------+---+\n|Count|Mark - Model - Color| rn|\n+-----+--------------------+---+\n|    6|  (SUNNY,NINGBO,RED)|  1|\n|    5|(ACURA,INTEGRA,BL...|  2|\n|    5|(CHRYSLER,SEBRING...|  3|\n|    5|(FORD,EXPLORER,GR...|  4|\n|    5|  (HONDA,CIVIC,GRAY)|  5|\n|    5|  (MITS,LANCER,GRAY)|  6|\n|    4|(CHRYSLER,PACIFIC...|  7|\n|    4|  (DODGE,DAKOTA,RED)|  8|\n|    4|  (HONDA,PILOT,GRAY)|  9|\n|    4|   (KIA,TRUCK,BLACK)| 10|\n|    3|   (ACUR,RDX,SILVER)| 11|\n|    3| (HONDA,ACCORD,BLUE)| 12|\n|    3|(INFINITI,SEDAN,G...| 13|\n|    3| (TOYO,CAMRY,SILVER)| 14|\n|    3|(TOYOTA,COROLLA,S...| 15|\n|    3|(TOYOTA,COROLLA,TAN)| 16|\n|    2|     (CHEV,SU,BLACK)| 17|\n|    2|(FORD,EXPEDITION,...| 18|\n|    2|     (HOND,4S,BLACK)| 19|\n|    2| (HONDA,CIVIC,BLACK)| 20|\n+-----+--------------------+---+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1541421639789_-706959464","id":"20181102-082256_1123923530","dateCreated":"2018-11-05T12:40:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1700","user":"anonymous","dateFinished":"2018-11-05T12:46:02+0000","dateStarted":"2018-11-05T12:45:49+0000"},{"text":"//aplicamos otro ROW_NUMBER, pero esta vez sobre el dataframe donde previamente hemos aplicado el DENSE_RANK()\n// para seleccionar las primeras filas de cada conjunto de datos en el Count\nval window_RN_over_DR = Window.partitionBy(col(\"dr\")).orderBy(col(\"dr\").desc)\nval dfData_RN_over_DR = dfDataDR.withColumn(\"rn\", row_number().over(window_RN_over_DR))\ndfData_RN_over_DR.show()","dateUpdated":"2018-11-05T12:45:21+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"window_RN_over_DR: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@6f9f39f2\ndfData_RN_over_DR: org.apache.spark.sql.DataFrame = [Count: int, Mark - Model - Color: string ... 2 more fields]\n+-----+--------------------+---+---+\n|Count|Mark - Model - Color| dr| rn|\n+-----+--------------------+---+---+\n|    6|  (SUNNY,NINGBO,RED)|  1|  1|\n|    5|(ACURA,INTEGRA,BL...|  2|  1|\n|    5|(CHRYSLER,SEBRING...|  2|  2|\n|    5|(FORD,EXPLORER,GR...|  2|  3|\n|    5|  (HONDA,CIVIC,GRAY)|  2|  4|\n|    5|  (MITS,LANCER,GRAY)|  2|  5|\n|    4|(CHRYSLER,PACIFIC...|  3|  1|\n|    4|  (DODGE,DAKOTA,RED)|  3|  2|\n|    4|  (HONDA,PILOT,GRAY)|  3|  3|\n|    4|   (KIA,TRUCK,BLACK)|  3|  4|\n|    3|   (ACUR,RDX,SILVER)|  4|  1|\n|    3| (HONDA,ACCORD,BLUE)|  4|  2|\n|    3|(INFINITI,SEDAN,G...|  4|  3|\n|    3| (TOYO,CAMRY,SILVER)|  4|  4|\n|    3|(TOYOTA,COROLLA,S...|  4|  5|\n|    3|(TOYOTA,COROLLA,TAN)|  4|  6|\n|    2|     (CHEV,SU,BLACK)|  5|  1|\n|    2|(FORD,EXPEDITION,...|  5|  2|\n|    2|     (HOND,4S,BLACK)|  5|  3|\n|    2| (HONDA,CIVIC,BLACK)|  5|  4|\n+-----+--------------------+---+---+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1541421639789_-706959464","id":"20181102-082313_905754531","dateCreated":"2018-11-05T12:40:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1701","user":"anonymous","dateFinished":"2018-11-05T12:46:09+0000","dateStarted":"2018-11-05T12:45:57+0000"},{"text":"//nos quedamos solo con las filas donde rn sea igual a 1 para despreciar el resto\nval dfData_RN_over_DR_filtered = dfData_RN_over_DR.filter(dfData_RN_over_DR(\"rn\") === 1).drop(\"rn\").drop(\"dr\")\ndfData_RN_over_DR_filtered.show()\n\n/*\n#primeras ocurrencias de cada conjunto de número de accidentes:\n#ejemplo de lo que debe mostrar:\n#+-----+--------------------+---+---+\n#|Count|Mark - Model - Color| dr| rn|\n#+-----+--------------------+---+---+\n#|    6|  [SUNNY,NINGBO,RED]|  1|  1|<---\n#|    5|[ACURA,INTEGRA,BL...|  2|  1|<---\n#|    5|[CHRYSLER,SEBRING...|  2|  2|\n#|    5|[FORD,EXPLORER,GR...|  2|  3|\n#|    5|  [HONDA,CIVIC,GRAY]|  2|  4|\n#|    5|  [MITS,LANCER,GRAY]|  2|  5|\n#|    4|[CHRYSLER,PACIFIC...|  3|  1|<---\n#|    4|  [DODGE,DAKOTA,RED]|  3|  2|\n#|    4|  [HONDA,PILOT,GRAY]|  3|  3|\n#|    4|   [KIA,TRUCK,BLACK]|  3|  4|\n#|    3|   [ACUR,RDX,SILVER]|  4|  1|<---\n#|    3| [HONDA,ACCORD,BLUE]|  4|  2|\n#|    3|[INFINITI,SEDAN,G...|  4|  3|\n#|    3| [TOYO,CAMRY,SILVER]|  4|  4|\n#|    3|[TOYOTA,COROLLA,S...|  4|  5|\n#|    3|[TOYOTA,COROLLA,TAN]|  4|  6|\n#|    2|     [CHEV,SU,BLACK]|  5|  1|<---\n#|    2|[FORD,EXPEDITION,...|  5|  2|\n#|    2|     [HOND,4S,BLACK]|  5|  3|\n#|    2| [HONDA,CIVIC,BLACK]|  5|  4|\n#+-----+--------------------+---+---+\n*/","dateUpdated":"2018-11-05T12:45:21+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"dfData_RN_over_DR_filtered: org.apache.spark.sql.DataFrame = [Count: int, Mark - Model - Color: string]\n+-----+--------------------+\n|Count|Mark - Model - Color|\n+-----+--------------------+\n|    6|  (SUNNY,NINGBO,RED)|\n|    5|(ACURA,INTEGRA,BL...|\n|    4|(CHRYSLER,PACIFIC...|\n|    3|   (ACUR,RDX,SILVER)|\n|    2|     (CHEV,SU,BLACK)|\n|    1|    (ACURA,2D,GREEN)|\n+-----+--------------------+\n\n"}]},"apps":[],"jobName":"paragraph_1541421639790_-705805218","id":"20181102-082329_863216843","dateCreated":"2018-11-05T12:40:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1702","user":"anonymous","dateFinished":"2018-11-05T12:46:15+0000","dateStarted":"2018-11-05T12:46:02+0000"},{"text":"//para establecer un ranking en base al numero de ocurrencias y poder mostrar despues\n//del join los 3 primeros coches con mas accidentes\n\nval rankingSequence = Seq(Row(1),Row(2),Row(3))\nval schema = List(StructField(\"rn\", IntegerType, true))\nval dfNew2 = spark.createDataFrame(spark.sparkContext.parallelize(rankingSequence), StructType(schema))\n\ndfNew2.show()","dateUpdated":"2018-11-05T12:45:21+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"rankingSequence: Seq[org.apache.spark.sql.Row] = List([1], [2], [3])\nschema: List[org.apache.spark.sql.types.StructField] = List(StructField(rn,IntegerType,true))\ndfNew2: org.apache.spark.sql.DataFrame = [rn: int]\n+---+\n| rn|\n+---+\n|  1|\n|  2|\n|  3|\n+---+\n\n"}]},"apps":[],"jobName":"paragraph_1541421639790_-705805218","id":"20181102-082341_1748973300","dateCreated":"2018-11-05T12:40:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1703","user":"anonymous","dateFinished":"2018-11-05T12:46:23+0000","dateStarted":"2018-11-05T12:46:09+0000"},{"text":"//hacemos el join de los dos dataframes llamando a la funcion joinDataFrames\n//y cruzando por la columna \"rn\" para que nos saque los 3 coches con mas\n//accidentes\nval DF_join = joinDataFrames(dfDataRN,dfNew2,JOIN_COLUMN)\n\n\n//renombramos la columna\nDF_join.orderBy(JOIN_COLUMN).withColumnRenamed(JOIN_COLUMN, \"Ranking\").show()\n\n/*\n#ejemplo de lo que debe mostrar:\n#+-----+--------------------+---+\n#|Count|Mark - Model - Color| rn|\n#+-----+--------------------+---+\n#|    6|  [SUNNY,NINGBO,RED]|  1|<---\n#|    5|[ACURA,INTEGRA,BL...|  2|<---\n#|    5|[CHRYSLER,SEBRING...|  3|<---\n#|    5|[FORD,EXPLORER,GR...|  4|\n#|    5|  [HONDA,CIVIC,GRAY]|  5|\n#|    5|  [MITS,LANCER,GRAY]|  6|\n*/\n","dateUpdated":"2018-11-05T12:45:21+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"DF_join: org.apache.spark.sql.DataFrame = [rn: int, Count: int ... 1 more field]\n+-------+-----+--------------------+\n|Ranking|Count|Mark - Model - Color|\n+-------+-----+--------------------+\n|      1|    6|  (SUNNY,NINGBO,RED)|\n|      2|    5|(ACURA,INTEGRA,BL...|\n|      3|    5|(CHRYSLER,SEBRING...|\n+-------+-----+--------------------+\n\n"}]},"apps":[],"jobName":"paragraph_1541421639791_-706189966","id":"20181102-082416_75236995","dateCreated":"2018-11-05T12:40:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1704","user":"anonymous","dateFinished":"2018-11-05T12:46:30+0000","dateStarted":"2018-11-05T12:46:16+0000"},{"dateUpdated":"2018-11-05T12:40:39+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1541421639791_-706189966","id":"20181102-082435_560927107","dateCreated":"2018-11-05T12:40:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1705"}],"name":"scala RDD notebook","id":"2DX5QUS21","angularObjects":{"2DVHQZ56U:shared_process":[],"2DVJ75R1H:shared_process":[],"2DW9N4N52:shared_process":[],"2DUD6VZ51:shared_process":[],"2DV18YM66:shared_process":[],"2DWNQKBH1:shared_process":[],"2DUJYV766:shared_process":[],"2DWXDGMPS:shared_process":[],"2DUJVU56U:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}